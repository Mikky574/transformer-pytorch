{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c99d33",
   "metadata": {},
   "source": [
    "# 编码器写太长了，分出来写解码器吧\n",
    "\n",
    "也可以再复习一下整个的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e0fee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 46, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前面的其实就到完成了MultiHeadAttention的向前传播的\n",
    "# 这块用来把数据给接上，把数据集读出来并得到编码器跑出来的结果，作为下一步的解码器的需要\n",
    "\n",
    "from datasets import IndexedInputTargetTranslationDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pipe import input_target_collate_fn\n",
    "\n",
    "config={\n",
    "    'data_dir':\"data/example/processed\",\n",
    "    'save_config':\"checkpoints/example_config.json\",\n",
    "    'save_checkpoint':\"checkpoints/example_model.pth\",\n",
    "    'save_log':\"logs/example.log\",\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'print_every': 1,\n",
    "    'save_every': 1,\n",
    "    'vocabulary_size': None,\n",
    "    'd_model': 128,\n",
    "    'layers_count': 1,\n",
    "    'heads_count': 2,\n",
    "    'd_ff': 128,\n",
    "    'dropout_prob': 0.1,\n",
    "    'label_smoothing': 0.1,\n",
    "    'optimizer': \"Adam\",\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'positional_encoding': True,\n",
    "    'clip_grads': False,\n",
    "    'dataset_limit':None\n",
    "}\n",
    "\n",
    "train_dataset = IndexedInputTargetTranslationDataset(\n",
    "    data_dir=config['data_dir'],\n",
    "    phase='train',\n",
    "    vocabulary_size=config['vocabulary_size'],\n",
    "    limit=config['dataset_limit'])\n",
    "\n",
    "val_dataset = IndexedInputTargetTranslationDataset(\n",
    "    data_dir=config['data_dir'],\n",
    "    phase='val',\n",
    "    vocabulary_size=config['vocabulary_size'],\n",
    "    limit=config['dataset_limit'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "#     shuffle=True, #研究怎么算的时候，就简单点，不随机了，而且还只取一小块来算\n",
    "    shuffle=False,\n",
    "    collate_fn=input_target_collate_fn)\n",
    "#DataLoader的collate_fn函数是来实现序列padding的操作。#输入是batch，输出是batch中的tensor数据\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    collate_fn=input_target_collate_fn)\n",
    "\n",
    "#因为前面生成的是迭代器，所以这么去取一个batch的数据\n",
    "import itertools\n",
    "\n",
    "n_items = 1  # 希望只取第一项\n",
    "batch_iter = iter(train_dataloader)\n",
    "for i, batch in itertools.islice(enumerate(batch_iter), n_items):\n",
    "    data_ba1=batch\n",
    "\n",
    "##参考这个\n",
    "# class Transformer(nn.Module):\n",
    "\n",
    "#     def __init__(self, encoder, decoder):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "\n",
    "#     def forward(self, sources, inputs):\n",
    "#         # sources : (batch_size, sources_len)\n",
    "#         # inputs : (batch_size, targets_len - 1)\n",
    "#         batch_size, sources_len = sources.size()\n",
    "#         batch_size, inputs_len = inputs.size()\n",
    "\n",
    "#         sources_mask = pad_masking(sources, sources_len)\n",
    "#         memory_mask = pad_masking(sources, inputs_len)\n",
    "#         inputs_mask = subsequent_masking(inputs) | pad_masking(inputs, inputs_len)\n",
    "\n",
    "#         memory = self.encoder(sources, sources_mask)  # (batch_size, seq_len, d_model)\n",
    "#         outputs, state = self.decoder(inputs, memory, memory_mask, inputs_mask)  # (batch_size, seq_len, d_model)\n",
    "#         return outputs\n",
    "\n",
    "from models import TransformerEncoder\n",
    "from embeddings import PositionalEncoding\n",
    "from dictionaries import IndexDictionary\n",
    "\n",
    "source_dictionary = IndexDictionary.load(config['data_dir'], mode='source', vocabulary_size=config['vocabulary_size'])\n",
    "target_dictionary = IndexDictionary.load(config['data_dir'], mode='target', vocabulary_size=config['vocabulary_size'])\n",
    "\n",
    "source_vocabulary_size,target_vocabulary_size=source_dictionary.vocabulary_size, target_dictionary.vocabulary_size\n",
    "\n",
    "source_embedding = PositionalEncoding(\n",
    "    num_embeddings=source_vocabulary_size,\n",
    "    embedding_dim=config['d_model'],\n",
    "    dim=config['d_model'])  # why dim?\n",
    "target_embedding = PositionalEncoding(\n",
    "    num_embeddings=target_vocabulary_size,\n",
    "    embedding_dim=config['d_model'],\n",
    "    dim=config['d_model'])  # why dim?\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    layers_count=config['layers_count'],\n",
    "    d_model=config['d_model'],\n",
    "    heads_count=config['heads_count'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout_prob=config['dropout_prob'],\n",
    "    embedding=source_embedding)\n",
    "\n",
    "#还缺个mask\n",
    "from utils.pad import pad_masking\n",
    "\n",
    "sources=batch[0]\n",
    "batch_size, sources_len = sources.size()\n",
    "sources_mask = pad_masking(sources, sources_len)\n",
    "\n",
    "memory = encoder(sources, sources_mask)#这里，是把这个结果叫做memory，一种记忆\n",
    "memory.shape#memory就是那一个箭号了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a17ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        outputs, state = self.decoder(inputs, memory, memory_mask, inputs_mask)\n",
    "#到这一步了，我接下来需要理解的就是这段的代码，这个decoder具体是做了什么，只有有了更加明晰的概念，才能逐渐明白这一切\n",
    "\n",
    "#继续啃吧，孤独还是生活的主旋律，没有人会一直陪你聊天的，而且还是在聊你自己研究领域的模型，更多的是已读不回，\n",
    "#更常见的是收拾自己的沮丧，表达出没关系的感觉，始终有着遥远的距离感，可望而不可即。\n",
    "import torch\n",
    "from torch import nn\n",
    "from models import Sublayer,LayerNormalization,PointwiseFeedForwardNetwork\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers_count, d_model, heads_count, d_ff, dropout_prob, embedding):\n",
    "        #这里的d_ff就是PointwiseFeedForwardNetwork中间的层，做一点中间的变化罢了。其实并不那么重要，值为128，\n",
    "        #只是帮助网络变得复杂一点罢了\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model#就是128\n",
    "        self.embedding = embedding#德语做的嵌入层\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [TransformerDecoderLayer(d_model, heads_count, d_ff, dropout_prob) for _ in range(layers_count)]\n",
    "        )#译码器的主要的层\n",
    "        #heads_count值为2，d_ff为128\n",
    "        self.generator = nn.Linear(embedding.embedding_dim, embedding.num_embeddings)\n",
    "        #这两个值为128与 35820，看上去就是一层映射，映射为生成的德语词的概率\n",
    "        self.generator.weight = self.embedding.weight#改为嵌入层的权重\n",
    "        \"\"\"\n",
    "        这里线性层的权重改成了嵌入层的权重，那么在向前传播的运算当中的线性层运算就是在用嵌入层的权重来计算\n",
    "        在这段代码中，`self.embedding`是输入词汇的嵌入层，而`self.generator`是输出词汇的线性层（也可以称为投影层），\n",
    "        在训练过程中，根据损失函数计算的梯度，会先优化`self.generator`的权重，再更新`self.embedding`的权重，\n",
    "        因此嵌入层的权重确实被优化了两次。这种方法通常被称为共享嵌入层（shared embedding）。\n",
    "        这种做法有助于减少模型参数的数量，以及提高其泛化能力。\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, inputs, memory, memory_mask, inputs_mask=None, state=None):\n",
    "        # inputs: (batch_size, seq_len - 1, d_model)\n",
    "        # memory: (batch_size, seq_len, d_model)\n",
    "\n",
    "        inputs = self.embedding(inputs)#这里关注的是inputs，这个输入应该是掩码后的含有开始符的德文内容\n",
    "        #经过嵌入层后张成128维的嵌入层数据\n",
    "        # if state is not None:\n",
    "        #     inputs = torch.cat([state.previous_inputs, inputs], dim=1)\n",
    "        #\n",
    "        #     state.previous_inputs = inputs\n",
    "\n",
    "        for layer_index, decoder_layer in enumerate(self.decoder_layers):#啊哈，为什么这里有两个东西，完全不理解，23333\n",
    "            # 哦哦哦，忘记了，才看到是enumerate迭代器，这种迭代器的写法，会返回两个东西，一个是索引的序号，另一个才是本身的内容\n",
    "            if state is None:\n",
    "                inputs = decoder_layer(inputs, memory, memory_mask, inputs_mask)# 就用到这个decoder本身，就跑了这一步\n",
    "            else: # Use cache\n",
    "                layer_cache = state.layer_caches[layer_index]# 呃，这里用上了layer_index\n",
    "                # print('inputs_mask', inputs_mask)\n",
    "                inputs = decoder_layer(inputs, memory, memory_mask, inputs_mask, layer_cache)\n",
    "\n",
    "                state.update_state(\n",
    "                    layer_index=layer_index,\n",
    "                    layer_mode='self-attention',\n",
    "                    key_projected=decoder_layer.self_attention_layer.sublayer.key_projected,\n",
    "                    value_projected=decoder_layer.self_attention_layer.sublayer.value_projected,\n",
    "                )\n",
    "                state.update_state(\n",
    "                    layer_index=layer_index,\n",
    "                    layer_mode='memory-attention',\n",
    "                    key_projected=decoder_layer.memory_attention_layer.sublayer.key_projected,\n",
    "                    value_projected=decoder_layer.memory_attention_layer.sublayer.value_projected,\n",
    "                )\n",
    "\n",
    "        generated = self.generator(inputs)  # (batch_size, seq_len, vocab_size)\n",
    "        #把译码器的结果映射为德语词向量大小的那啥的维度，估计是走一步归一化，然后变为每个词的概率\n",
    "        return generated, state #state还没有看到有什么作用\n",
    "\n",
    "    def init_decoder_state(self, **args):\n",
    "        return DecoderState()\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads_count, d_ff, dropout_prob):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention_layer = Sublayer(MultiHeadAttention(heads_count, d_model, dropout_prob, mode='self-attention'), d_model)\n",
    "        #就只有模式的区别\n",
    "        self.memory_attention_layer = Sublayer(MultiHeadAttention(heads_count, d_model, dropout_prob, mode='memory-attention'), d_model)\n",
    "        #这个不太一样，用的是记忆注意力，有点没研究过，还得把多头注意力摆出来研究研究\n",
    "        self.pointwise_feedforward_layer = Sublayer(PointwiseFeedForwardNetwork(d_ff, d_model, dropout_prob), d_model)\n",
    "\n",
    "    def forward(self, inputs, memory, memory_mask, inputs_mask, layer_cache=None):#这里好像有cache，但有一说一，没看到cache的调用\n",
    "        #应该正常时没有用到这一块的cache的，就不用管了\n",
    "        # print('self attention')\n",
    "        # print('inputs_mask', inputs_mask)\n",
    "        inputs = self.self_attention_layer(inputs, inputs, inputs, inputs_mask, layer_cache)#自注意力\n",
    "        # print('memory attention')\n",
    "        inputs = self.memory_attention_layer(inputs, memory, memory, memory_mask, layer_cache)#记忆注意力\n",
    "        inputs = self.pointwise_feedforward_layer(inputs)#非线性变化\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads_count, d_model, dropout_prob, mode='self-attention'):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % heads_count == 0\n",
    "        assert mode in ('self-attention', 'memory-attention')\n",
    "\n",
    "        self.d_head = d_model // heads_count\n",
    "        self.heads_count = heads_count\n",
    "        self.mode = mode\n",
    "        self.query_projection = nn.Linear(d_model, heads_count * self.d_head)#q矩阵\n",
    "        self.key_projection = nn.Linear(d_model, heads_count * self.d_head)  #k矩阵\n",
    "        self.value_projection = nn.Linear(d_model, heads_count * self.d_head)#v矩阵\n",
    "        self.final_projection = nn.Linear(d_model, heads_count * self.d_head)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.softmax = nn.Softmax(dim=3)\n",
    "\n",
    "        self.attention = None\n",
    "        # For cache\n",
    "        self.key_projected = None\n",
    "        self.value_projected = None\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, layer_cache=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            query: (batch_size, query_len, model_dim)\n",
    "            key: (batch_size, key_len, model_dim)\n",
    "            value: (batch_size, value_len, model_dim)\n",
    "            mask: (batch_size, query_len, key_len)\n",
    "            state: DecoderState\n",
    "        \"\"\"\n",
    "        # print('attention mask', mask)\n",
    "        batch_size, query_len, d_model = query.size()\n",
    "\n",
    "        d_head = d_model // self.heads_count\n",
    "\n",
    "        query_projected = self.query_projection(query)\n",
    "        # print('query_projected', query_projected.shape)\n",
    "        if layer_cache is None or layer_cache[self.mode] is None:  # Don't use cache\n",
    "            #应该还是用的这个，因为没看到layer_cache\n",
    "            key_projected = self.key_projection(key) \n",
    "            value_projected = self.value_projection(value)\n",
    "        else:  # Use cache\n",
    "            if self.mode == 'self-attention':#自注意力机制\n",
    "                key_projected = self.key_projection(key)\n",
    "                value_projected = self.value_projection(value)\n",
    "\n",
    "                key_projected = torch.cat([key_projected, layer_cache[self.mode]['key_projected']], dim=1)#嘶，用上了拼接，这块没研究过\n",
    "                value_projected = torch.cat([value_projected, layer_cache[self.mode]['value_projected']], dim=1)\n",
    "            elif self.mode == 'memory-attention':#记忆注意力机制\n",
    "                key_projected = layer_cache[self.mode]['key_projected']\n",
    "                value_projected = layer_cache[self.mode]['value_projected']\n",
    "\n",
    "        # For cache\n",
    "        self.key_projected = key_projected\n",
    "        self.value_projected = value_projected\n",
    "\n",
    "        batch_size, key_len, d_model = key_projected.size()\n",
    "        batch_size, value_len, d_model = value_projected.size()\n",
    "\n",
    "        query_heads = query_projected.view(batch_size, query_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, query_len, d_head)\n",
    "        # print('query_heads', query_heads.shape)\n",
    "        # print(batch_size, key_len, self.heads_count, d_head)\n",
    "        # print(key_projected.shape)\n",
    "        key_heads = key_projected.view(batch_size, key_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, key_len, d_head)\n",
    "        value_heads = value_projected.view(batch_size, value_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, value_len, d_head)\n",
    "\n",
    "        attention_weights = self.scaled_dot_product(query_heads, key_heads)  # (batch_size, heads_count, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            # print('mode', self.mode)\n",
    "            # print('mask', mask.shape)\n",
    "            # print('attention_weights', attention_weights.shape)\n",
    "            mask_expanded = mask.unsqueeze(1).expand_as(attention_weights)\n",
    "            attention_weights = attention_weights.masked_fill(mask_expanded, -1e18)\n",
    "\n",
    "        self.attention = self.softmax(attention_weights)  # Save attention to the object\n",
    "        # print('attention_weights', attention_weights.shape)\n",
    "        attention_dropped = self.dropout(self.attention)\n",
    "        context_heads = torch.matmul(attention_dropped, value_heads)  # (batch_size, heads_count, query_len, d_head)\n",
    "        # print('context_heads', context_heads.shape)\n",
    "        context_sequence = context_heads.transpose(1, 2).contiguous()  # (batch_size, query_len, heads_count, d_head)\n",
    "        context = context_sequence.view(batch_size, query_len, d_model)  # (batch_size, query_len, d_model)\n",
    "        final_output = self.final_projection(context)\n",
    "        # print('final_output', final_output.shape)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "class DecoderState:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.previous_inputs = torch.tensor([])\n",
    "        self.layer_caches = defaultdict(lambda: {'self-attention': None, 'memory-attention': None})\n",
    "\n",
    "    def update_state(self, layer_index, layer_mode, key_projected, value_projected):\n",
    "        self.layer_caches[layer_index][layer_mode] = {\n",
    "            'key_projected': key_projected,\n",
    "            'value_projected': value_projected\n",
    "        }\n",
    "\n",
    "    # def repeat_beam_size_times(self, beam_size): # memory만 repeat하면 되는데 state에 memory는 넣지 않기로 했다.\n",
    "    #     self.\n",
    "    #     self.src = self.src.data.repeat(beam_size, 1)\n",
    "\n",
    "    def beam_update(self, positions):\n",
    "        for layer_index in self.layer_caches:\n",
    "            for mode in ('self-attention', 'memory-attention'):\n",
    "                if self.layer_caches[layer_index][mode] is not None:\n",
    "                    for projection in self.layer_caches[layer_index][mode]:\n",
    "                        cache = self.layer_caches[layer_index][mode][projection]\n",
    "                        if cache is not None:\n",
    "                            cache.data.copy_(cache.data.index_select(0, positions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "641796da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#译码器，满理解一下这个embedding，就从embedding开始理解这个译码器吧\n",
    "decoder = TransformerDecoder(\n",
    "    layers_count=config['layers_count'],\n",
    "    d_model=config['d_model'],\n",
    "    heads_count=config['heads_count'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout_prob=config['dropout_prob'],\n",
    "    embedding=target_embedding)\n",
    "#借以参照初始化的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "779fa156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEncoding(\n",
       "  (embbedding): Embedding(35820, 128, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding #这个就是按照德语的词表建立一个嵌入层，只是数据集不一样罢了\n",
    "#很快就能理解了，可以往下走"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "924a7ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 35820)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding.embedding_dim, target_embedding.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f18ff54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49])\n"
     ]
    }
   ],
   "source": [
    "inputs=batch[1]\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ef27bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49, 128])\n"
     ]
    }
   ],
   "source": [
    "#         memory = self.encoder(sources, sources_mask)  # (batch_size, seq_len, d_model)\n",
    "#         outputs, state = self.decoder(inputs, memory, memory_mask, inputs_mask)  # (batch_size, seq_len, d_model)\n",
    "#译码器跑的时候，向前传播的参数是这个\n",
    "inputs_embedding=target_embedding(inputs)\n",
    "print(inputs_embedding.shape)\n",
    "#嵌入层之后，就到译码器了输入为inputs, memory, memory_mask, inputs_mask\n",
    "#这里建立这些变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b4c148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, inputs_len = inputs.size()\n",
    "import numpy as np\n",
    "def subsequent_masking(x):\n",
    "    # x: (batch_size, seq_len - 1)\n",
    "    batch_size, seq_len = x.size()\n",
    "    subsequent_mask = np.triu(np.ones(shape=(seq_len, seq_len)), k=1).astype('uint8')\n",
    "    subsequent_mask = torch.tensor(subsequent_mask).to(x.device)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(batch_size, seq_len, seq_len)\n",
    "    return subsequent_mask\n",
    "#要不要理解这一块代码呢，今天有点发懒，后边再说吧\n",
    "memory_mask = pad_masking(sources, inputs_len)\n",
    "inputs_mask = subsequent_masking(inputs) | pad_masking(inputs, inputs_len)\n",
    "#懂了，看这一段使用的效果，就能理解了这一块代码做了什么\n",
    "print(memory_mask.shape,inputs_mask.shape)#主要看一下这个inputs_mask的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "934d9804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mask[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10c71b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mask[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9617e87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mask[0][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd992b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11,  8, 12, 13,  8, 14, 15, 16, 17,  6,\n",
       "        18, 19,  8, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]#和这个对比，可以知道是在哪里提前停下来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6d415a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49, 128]) torch.Size([64, 46, 128]) torch.Size([64, 49, 46]) torch.Size([64, 49, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 49, 128])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#结合这一段去理解，能看出inputs的mask就是一点点的移掉掩码的，最后到inputs的结束符为止，掩码就不移动了\n",
    "print(inputs_embedding.shape,memory.shape,memory_mask.shape,inputs_mask.shape)#所需的东西都在这里了\n",
    "#开始步进，进入到decoder里面去\n",
    "#         inputs = self.self_attention_layer(inputs, inputs, inputs, inputs_mask, layer_cache)#自注意力\n",
    "#self.self_attention_layer = Sublayer(MultiHeadAttention(heads_count, d_model, dropout_prob, mode='self-attention'), d_model)\n",
    "#自注意力机制的向前传播，进入到子注意力机制里面去，不过之前研究过，就是一个q,k,v的变换\n",
    "#从一个嵌入层的词向量q矩阵，点乘以同样意义的k矩阵的转置后，得到的行向量是单个字与每个字的相关性；然后q和k矩阵点乘后，再与同样意义的v矩阵点乘\n",
    "#，就会得到行向量是单个字与每个字的相关性在第1维度到第128维度的特征。就相当于巧妙地转回来了，同时这个向量不单单只与自己有关，\n",
    "#而是与每个词相关了。\n",
    "\n",
    "inputs_self_att=MultiHeadAttention(heads_count=2, d_model=128, dropout_prob=0.1, mode='self-attention')(inputs_embedding,inputs_embedding,inputs_embedding,inputs_mask)\n",
    "print(inputs_self_att.shape)#得到带层层递进的掩码的input输入的自注意力的词相关矩阵\n",
    "#Sublayer是在做残差，对整体的理解可以跳过这一步，就走主要的机制，来试着理解这个模型的整体的变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "daf8091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49, 128]) torch.Size([64, 46, 128])\n",
      "torch.Size([64, 49, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 49, 128])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs_self_att.shape,memory.shape)\n",
    "#这里的mode其实没影响，因为cache是None，做一样的动作，但q,k,v的输入不一样了，这样整体的意义的变化也会有不一样\n",
    "inputs_memo_att=MultiHeadAttention(heads_count=2, d_model=128, dropout_prob=0.1, mode='memory-attention')(inputs_self_att,memory,memory,memory_mask)\n",
    "print(inputs_memo_att.shape)#这样德字和英字就完全关联上了。按照之前的绘图理解了一遍，算是通透\n",
    "\n",
    "decoder_out=PointwiseFeedForwardNetwork(128, 128, 0.1)(inputs_memo_att)#非线性变化，加深网络深度\n",
    "decoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "19363594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 49, 35820])\n"
     ]
    }
   ],
   "source": [
    "#最后一步，生成最后的输出结果\n",
    "\n",
    "generator = nn.Linear(128,target_vocabulary_size)\n",
    "generated=generator(decoder_out)\n",
    "print(generated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a865c7",
   "metadata": {},
   "source": [
    "# 至此，模型部分全部结束，只需要再理解损失函数是怎么计算的，就算是从代码层面完全理解了transformer项目\n",
    "\n",
    "有一说一，整体看下来，在没有人带着学的情况下，慢慢找材料，逐渐看懂模型，确实是个不容易的事情。但看起来以transformer作为入门模型来学习的路线还是可行的。\n",
    "\n",
    "呼，这样就完成了整体的3/4了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df2376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
