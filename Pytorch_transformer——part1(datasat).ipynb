{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69fb43d",
   "metadata": {},
   "source": [
    "# 对pytorch的transformer的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a3b95",
   "metadata": {},
   "source": [
    "# 设置的几个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994cfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>'#变成编码后，应该填充是0\n",
    "UNK_TOKEN = '<UNK>'#未知是1\n",
    "START_TOKEN = '<StartSent>'#开始是2\n",
    "END_TOKEN = '<EndSent>'#结束是3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98901a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import dirname,abspath\n",
    "# BASE_DIR = dirname(abspath(__file__))\n",
    "# BASE_DIR\n",
    "from os.path import dirname, abspath, join, exists\n",
    "import os\n",
    "BASE_DIR = os.getcwd() # 等价于这个，那种表示适用于脚本中获取当前的路径的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d94b4",
   "metadata": {},
   "source": [
    "## 运行的代码\n",
    "```\n",
    "python prepare_datasets.py --train_source=data/example/raw/src-train.txt --train_target=data/example/raw/tgt-train.txt --val_source=data/example/raw/src-val.txt --val_target=data/example/raw/tgt-val.txt --save_data_dir=data/example/processed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2057fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source=\"data/example/raw/src-train.txt\"\n",
    "train_target=\"data/example/raw/tgt-train.txt\"\n",
    "val_source=\"data/example/raw/src-val.txt\"\n",
    "val_target=\"data/example/raw/tgt-val.txt\"\n",
    "save_data_dir=\"data/example/processed\"\n",
    "\n",
    "# 也要学一下这种表达\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser('Prepare datasets')\n",
    "# parser.add_argument('--train_source', type=str, default='data/example/raw/src-train.txt')\n",
    "# parser.add_argument('--train_target', type=str, default='data/example/raw/tgt-train.txt')\n",
    "# parser.add_argument('--val_source', type=str, default='data/example/raw/src-val.txt')\n",
    "# parser.add_argument('--val_target', type=str, default='data/example/raw/tgt-val.txt')\n",
    "# parser.add_argument('--save_data_dir', type=str, default='data/example/processed')\n",
    "# parser.add_argument('--share_dictionary', type=bool, default=False)\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset:\n",
    "\n",
    "    def __init__(self, data_dir, phase, limit=None):\n",
    "        assert phase in ('train', 'val'), \"Dataset phase must be either 'train' or 'val'\"\n",
    "\n",
    "        self.limit = limit\n",
    "\n",
    "        self.data = []\n",
    "        with open(join(data_dir, f'raw-{phase}.txt'), encoding='utf-8') as file:s\n",
    "            #这里是save_data_dir的路径下，读取的事处理后的数据集\n",
    "            for line in file:\n",
    "                source, target = line.strip().split('\\t')#source是这里面的英语\n",
    "                self.data.append((source, target))#target是德语\n",
    "            #训练集和测试集的数据都保存在data内了\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.limit is not None and item >= self.limit:\n",
    "            #获取元素的时候，这应该事索引，检查索引是否在人为设定的limit内\n",
    "            raise IndexError()\n",
    "\n",
    "        return self.data[item]\n",
    "        #按照索引返回元素，这应该是重写了Dataset类，在其他项目中也能看见这种写法\n",
    "        #要么取train，要么取val，只取一个\n",
    "        #这个是data配对起来的，读取的是处理后的文件\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.limit is None:\n",
    "            return len(self.data)\n",
    "        else:\n",
    "            return self.limit\n",
    "\n",
    "    @staticmethod #写成静态的方法，在调用的时候直接通过类来调用而不是需要设定实例\n",
    "    def prepare(train_source, train_target, val_source, val_target, save_data_dir):\n",
    "\n",
    "        if not exists(save_data_dir):\n",
    "            makedirs(save_data_dir) #生成保存处理后的数据的路径\n",
    "\n",
    "        for phase in ('train', 'val'):\n",
    "\n",
    "            if phase == 'train':\n",
    "                source_filepath = train_source\n",
    "                target_filepath = train_target\n",
    "            else:\n",
    "                source_filepath = val_source\n",
    "                target_filepath = val_target\n",
    "\n",
    "            with open(source_filepath, encoding='utf-8') as source_file:\n",
    "                #我默认的编码是gbk，会报错，直接指定字符集\n",
    "                source_data = source_file.readlines()# 按行读取，得到结果列表\n",
    "\n",
    "            with open(target_filepath, encoding='utf-8') as target_filepath:\n",
    "                target_data = target_filepath.readlines()\n",
    "\n",
    "            with open(join(save_data_dir, f'raw-{phase}.txt'), 'w', encoding='utf-8') as file:\n",
    "                for source_line, target_line in zip(source_data, target_data):#匹配生成数据和标签\n",
    "                    source_line = source_line.strip()#去掉换行符\n",
    "                    target_line = target_line.strip()\n",
    "                    line = f'{source_line}\\t{target_line}\\n'#以制表符为单位去区分数据和目标（label）\n",
    "                    file.write(line)\n",
    "        #这样之后，会生成处理好的文件“raw-train.txt”以及“raw-val.txt”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0029265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDatasetOnTheFly:\n",
    "    # TranslationDatasetOnTheFly是一个实时加载的数据集表示类，可以在调用getitem方法的时候动态地读取相应的数据文件。\n",
    "    # 这种实时加载的方式对于数据量比较大的数据集比较适用，可以减少在数据预处理时对内存的要求。\n",
    "\n",
    "    def __init__(self, phase, limit=None):\n",
    "        assert phase in ('train', 'val'), \"Dataset phase must be either 'train' or 'val'\"\n",
    "\n",
    "        self.limit = limit #默认限制为None\n",
    "\n",
    "        if phase == 'train':\n",
    "            source_filepath = join(BASE_DIR, 'data', 'example', 'raw', 'src-train.txt')# 读取文件的原始数据\n",
    "            target_filepath = join(BASE_DIR, 'data', 'example', 'raw', 'tgt-train.txt')\n",
    "        elif phase == 'val':\n",
    "            source_filepath = join(BASE_DIR, 'data', 'example', 'raw', 'src-val.txt')\n",
    "            target_filepath = join(BASE_DIR, 'data', 'example', 'raw', 'tgt-val.txt')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        with open(source_filepath, encoding='utf-8') as source_file:\n",
    "            self.source_data = source_file.readlines()\n",
    "\n",
    "        with open(target_filepath, encoding='utf-8') as target_filepath:\n",
    "            self.target_data = target_filepath.readlines()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.limit is not None and item >= self.limit:\n",
    "            #获取元素的时候，这应该事索引，检查索引是否在人为设定的limit内\n",
    "            raise IndexError()\n",
    "\n",
    "        source = self.source_data[item].strip()#去掉\n",
    "        target = self.target_data[item].strip()\n",
    "        return source, target\n",
    "        #没有配对起来，读取的是处理前的文件，但其实感觉也没什么区别。\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.limit is None:\n",
    "            return len(self.source_data)\n",
    "        else:\n",
    "            return self.limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf767b0",
   "metadata": {},
   "source": [
    "# @staticmethod静态语法糖举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415daede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a static method\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    @staticmethod\n",
    "    def my_static_method():\n",
    "        print(\"This is a static method\")\n",
    "\n",
    "MyClass.my_static_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a6286",
   "metadata": {},
   "source": [
    "# 来步进地去学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TranslationDataset.prepare(train_source, train_target, val_source, val_target, save_data_dir)#处理生成目标文件\n",
    "translation_dataset = TranslationDataset(save_data_dir, 'train')#读取train的数据集\n",
    "translation_dataset_on_the_fly = TranslationDatasetOnTheFly('train')#一样，读取train的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84597573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#也就来源不一样，一个需要数据预处理后才能运行，另一个不要，直接在源文件里面操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc24330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .',\n",
       " 'Es geht nicht an , dass über Ausführungsbestimmungen , deren Inhalt , Zweck und Ausmaß vorher nicht bestimmt ist , zusammen mit den nationalen Bürokratien das Gesetzgebungsrecht des Europäischen Parlaments ausgehebelt wird .')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f2b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .',\n",
       " 'Es geht nicht an , dass über Ausführungsbestimmungen , deren Inhalt , Zweck und Ausmaß vorher nicht bestimmt ist , zusammen mit den nationalen Bürokratien das Gesetzgebungsrecht des Europäischen Parlaments ausgehebelt wird .')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dataset_on_the_fly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491495f",
   "metadata": {},
   "source": [
    "# 先文字分割，取的时候切割字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9a30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedTranslationDataset:\n",
    "\n",
    "    def __init__(self, data_dir, phase, limit=None):\n",
    "\n",
    "        self.raw_dataset = TranslationDataset(data_dir, phase, limit)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        raw_source, raw_target = self.raw_dataset[item]\n",
    "        tokenized_source = raw_source.split()# 字符串切割，取的时候才切割的\n",
    "        tokenized_target = raw_target.split()\n",
    "        return tokenized_source, tokenized_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7484e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['It',\n",
       "  'is',\n",
       "  'not',\n",
       "  'acceptable',\n",
       "  'that',\n",
       "  ',',\n",
       "  'with',\n",
       "  'the',\n",
       "  'help',\n",
       "  'of',\n",
       "  'the',\n",
       "  'national',\n",
       "  'bureaucracies',\n",
       "  ',',\n",
       "  'Parliament',\n",
       "  '&apos;s',\n",
       "  'legislative',\n",
       "  'prerogative',\n",
       "  'should',\n",
       "  'be',\n",
       "  'made',\n",
       "  'null',\n",
       "  'and',\n",
       "  'void',\n",
       "  'by',\n",
       "  'means',\n",
       "  'of',\n",
       "  'implementing',\n",
       "  'provisions',\n",
       "  'whose',\n",
       "  'content',\n",
       "  ',',\n",
       "  'purpose',\n",
       "  'and',\n",
       "  'extent',\n",
       "  'are',\n",
       "  'not',\n",
       "  'laid',\n",
       "  'down',\n",
       "  'in',\n",
       "  'advance',\n",
       "  '.'],\n",
       " ['Es',\n",
       "  'geht',\n",
       "  'nicht',\n",
       "  'an',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'über',\n",
       "  'Ausführungsbestimmungen',\n",
       "  ',',\n",
       "  'deren',\n",
       "  'Inhalt',\n",
       "  ',',\n",
       "  'Zweck',\n",
       "  'und',\n",
       "  'Ausmaß',\n",
       "  'vorher',\n",
       "  'nicht',\n",
       "  'bestimmt',\n",
       "  'ist',\n",
       "  ',',\n",
       "  'zusammen',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'nationalen',\n",
       "  'Bürokratien',\n",
       "  'das',\n",
       "  'Gesetzgebungsrecht',\n",
       "  'des',\n",
       "  'Europäischen',\n",
       "  'Parlaments',\n",
       "  'ausgehebelt',\n",
       "  'wird',\n",
       "  '.'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = TokenizedTranslationDataset(save_data_dir, 'train')\n",
    "print(len(tokenized_dataset[0][0]))\n",
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67780898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.py`是一个工具模块，主要提供针对列表和迭代器的一些管道操作函数\n",
    "\n",
    "def source_tokens_generator(dataset):\n",
    "    for source, target in dataset:\n",
    "        for token in source:\n",
    "            yield token\n",
    "            #`yield`是一个关键字，可以在函数内部将函数包装为迭代器。\n",
    "\n",
    "def target_tokens_generator(dataset):\n",
    "    for source, target in dataset:\n",
    "        for token in target:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309d413c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 It\n",
      "1 is\n",
      "2 not\n",
      "3 acceptable\n",
      "4 that\n",
      "5 ,\n",
      "6 with\n",
      "7 the\n",
      "8 help\n",
      "9 of\n",
      "10 the\n",
      "11 national\n",
      "12 bureaucracies\n",
      "13 ,\n",
      "14 Parliament\n",
      "15 &apos;s\n",
      "16 legislative\n",
      "17 prerogative\n",
      "18 should\n",
      "19 be\n",
      "20 made\n",
      "21 null\n",
      "22 and\n",
      "23 void\n",
      "24 by\n",
      "25 means\n",
      "26 of\n",
      "27 implementing\n",
      "28 provisions\n",
      "29 whose\n",
      "30 content\n",
      "31 ,\n",
      "32 purpose\n",
      "33 and\n",
      "34 extent\n",
      "35 are\n",
      "36 not\n",
      "37 laid\n",
      "38 down\n",
      "39 in\n",
      "40 advance\n",
      "41 .\n",
      "42 Federal\n",
      "43 Master\n",
      "44 Trainer\n",
      "45 and\n",
      "46 Senior\n",
      "47 Instructor\n",
      "48 of\n",
      "49 the\n"
     ]
    }
   ],
   "source": [
    "source_generator = source_tokens_generator(tokenized_dataset)#生成一个迭代器\n",
    "source_generator\n",
    "\n",
    "#对迭代器的理解\n",
    "for i, item in enumerate(source_generator):#迭代器大概就是这个意思\n",
    "    if i < 50:\n",
    "        print(i, item)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6967c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# `Counter` 是一个 Python 内置类，用于统计可迭代对象中元素出现的次数。它返回一个字典，\n",
    "# 字典的 key 是元素，value 是对应元素出现的次数。\n",
    "\n",
    "class IndexDictionary:#词汇表类，用于生成词汇表\n",
    "\n",
    "    def __init__(self, iterable=None, mode='shared', vocabulary_size=None):\n",
    "\n",
    "        self.special_tokens = [PAD_TOKEN, UNK_TOKEN, START_TOKEN, END_TOKEN]\n",
    "        #填充字符，超出词表的字符，开始的字符，结束的字符\n",
    "        # On-the-fly mode\n",
    "        if iterable is not None:\n",
    "            #第一个读进来的参数时迭代器\n",
    "            self.vocab_tokens, self.token_counts = self._build_vocabulary(iterable, vocabulary_size)\n",
    "            #vocabulary_size是None，应该指的是无限制吧。获取到字符串的列表，也可以理解为做了一步set吧。还有一个是频数的列表\n",
    "            self.token_index_dict = {token: index for index, token in enumerate(self.vocab_tokens)}\n",
    "            #再将列表改为迭代器，主要是为了拿到索引，就是做了一步one-hot编码，\n",
    "            #最后的结果是，写了个词汇表字典，将字符符号映射为数字\n",
    "            self.vocabulary_size = len(self.vocab_tokens)\n",
    "            #词汇表的大小\n",
    "\n",
    "        self.mode = mode#就定义了模式\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        #用来映射字符串的，如果是没在词汇表里的字符串就映射为UNK，原来实际上是这么做的\n",
    "        try:\n",
    "            return self.token_index_dict[token]#从字典中，按照字符串取到映射的数字\n",
    "        except KeyError:\n",
    "            return self.token_index_dict[UNK_TOKEN]\n",
    "\n",
    "    def index_to_token(self, index):\n",
    "        if index >= self.vocabulary_size:\n",
    "            return self.vocab_tokens[UNK_TOKEN]\n",
    "        else:\n",
    "            return self.vocab_tokens[index]\n",
    "\n",
    "    def index_sentence(self, sentence):\n",
    "        # 紧接其后的IndexedInputTargetTranslationDataset就用到了这一块的内容\n",
    "        return [self.token_to_index(token) for token in sentence]\n",
    "        #一个列表的分割后的字符串进来，一堆映射的数字出去\n",
    "\n",
    "    def tokenify_indexes(self, token_indexes):\n",
    "        return [self.index_to_token(token_index) for token_index in token_indexes]\n",
    "\n",
    "    def _build_vocabulary(self, iterable, vocabulary_size):\n",
    "        #应该是建立一个词汇表\n",
    "        counter = Counter()#Counter类用来建立频数字典的\n",
    "        for token in iterable:\n",
    "            counter[token] += 1\n",
    "            #频数字典\n",
    "        if vocabulary_size is not None:\n",
    "            most_commons = counter.most_common(vocabulary_size - len(self.special_tokens))\n",
    "            frequent_tokens = [token for token, count in most_commons]\n",
    "            vocab_tokens = self.special_tokens + frequent_tokens\n",
    "            token_counts = [0] * len(self.special_tokens) + [count for token, count in most_commons]\n",
    "        else:\n",
    "            all_tokens = [token for token, count in counter.items()]#获得的是字典里的key，也就是字符本身\n",
    "            vocab_tokens = self.special_tokens + all_tokens#把那四个人为规定的字符加进去\n",
    "            token_counts = [0] * len(self.special_tokens) + [count for token, count in counter.items()]\n",
    "            #那这个获得的是频数的列表\n",
    "\n",
    "        return vocab_tokens, token_counts\n",
    "\n",
    "    def save(self, data_dir):\n",
    "\n",
    "        vocabulary_filepath = join(data_dir, f'vocabulary-{self.mode}.txt')#保存的路径\n",
    "        with open(vocabulary_filepath, 'w', encoding='utf-8') as file:\n",
    "            for vocab_index, (vocab_token, count) in enumerate(zip(self.vocab_tokens, self.token_counts)):\n",
    "                file.write(str(vocab_index) + '\\t' + vocab_token + '\\t' + str(count) + '\\n')\n",
    "                # 这样source是英语的词汇表，target是德语的词汇表\n",
    "\n",
    "    @classmethod #classmethod是用来指定一个类的方法为类方法，没有此参数指定的类的方法为实例方法，\n",
    "    #这样的好处就是你以后重构类的时候不必要修改构造函数，只需要额外添加你要处理的函数，然后使用\n",
    "    #装饰符 @classmethod 就可以了。\n",
    "    def load(cls, data_dir, mode='shared', vocabulary_size=None):\n",
    "        #python中cls代表的是类的本身，相对应的self则是类的一个实例对象。\n",
    "        #因为cls等同于类本身，类方法中可以通过使用cls来实例化一个对象。\n",
    "        vocabulary_filepath = join(data_dir, f'vocabulary-{mode}.txt')#文件\n",
    "\n",
    "        vocab_tokens = {}\n",
    "        token_counts = []\n",
    "        with open(vocabulary_filepath, encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                vocab_index, vocab_token, count = line.strip().split('\\t')\n",
    "                vocab_index = int(vocab_index)\n",
    "                vocab_tokens[vocab_index] = vocab_token\n",
    "                token_counts.append(int(count))#读取文件\n",
    "\n",
    "        if vocabulary_size is not None:\n",
    "            vocab_tokens = {k: v for k, v in vocab_tokens.items() if k < vocabulary_size}\n",
    "            token_counts = token_counts[:vocabulary_size]#截取特定长度的词汇表\n",
    "\n",
    "        instance = cls(mode=mode)#就是创了个新实例，原来如此\n",
    "        instance.vocab_tokens = vocab_tokens\n",
    "        instance.token_counts = token_counts\n",
    "        instance.token_index_dict = {token: index for index, token in vocab_tokens.items()}\n",
    "        instance.vocabulary_size = len(vocab_tokens)\n",
    "\n",
    "        return instance\n",
    "        #懂了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30e246a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_dictionary=False\n",
    "# 跑的是这段\n",
    "source_dictionary = IndexDictionary(source_generator, mode='source')\n",
    "#source_generator是个迭代器，这里的iterable为source_generator\n",
    "source_dictionary#这就只是个类而已，定义了很多的方法\n",
    "target_generator = target_tokens_generator(tokenized_dataset)\n",
    "target_dictionary = IndexDictionary(target_generator, mode='target')\n",
    "source_dictionary.save(save_data_dir)#这不用解释了，一目了然，保存为文件的代码\n",
    "target_dictionary.save(save_data_dir)\n",
    "\n",
    "#读取，其实只要这么读就可以了，处理完后只需要运行这一个就行了\n",
    "source_dictionary = IndexDictionary.load(save_data_dir, mode='source')\n",
    "target_dictionary = IndexDictionary.load(save_data_dir, mode='target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684ac4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24995, 35820)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dictionary = IndexDictionary.load(save_data_dir, mode='source')\n",
    "target_dictionary = IndexDictionary.load(save_data_dir, mode='target')\n",
    "\n",
    "source_dictionary.vocabulary_size,target_dictionary.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d7bfc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " '<StartSent>': 2,\n",
       " '<EndSent>': 3,\n",
       " 'Federation': 4,\n",
       " 'of': 5,\n",
       " 'Aerobic': 6,\n",
       " 'Fitness': 7,\n",
       " ',': 8,\n",
       " 'Group': 9,\n",
       " 'Postural': 10,\n",
       " 'Gym': 11,\n",
       " 'Stretching': 12,\n",
       " 'and': 13,\n",
       " 'Pilates;': 14,\n",
       " 'from': 15,\n",
       " '2004': 16,\n",
       " 'he': 17,\n",
       " 'has': 18,\n",
       " 'been': 19,\n",
       " 'collaborating': 20,\n",
       " 'with': 21,\n",
       " 'Antiche': 22,\n",
       " 'Terme': 23,\n",
       " 'as': 24,\n",
       " 'personal': 25,\n",
       " 'Trainer': 26,\n",
       " 'Instructor': 27,\n",
       " 'Pilates': 28,\n",
       " '.': 29,\n",
       " '&quot;': 30,\n",
       " 'Two': 31,\n",
       " 'soldiers': 32,\n",
       " 'came': 33,\n",
       " 'up': 34,\n",
       " 'to': 35,\n",
       " 'me': 36,\n",
       " 'told': 37,\n",
       " 'that': 38,\n",
       " 'if': 39,\n",
       " 'I': 40,\n",
       " 'refuse': 41,\n",
       " 'sleep': 42,\n",
       " 'them': 43,\n",
       " 'they': 44,\n",
       " 'will': 45,\n",
       " 'kill': 46,\n",
       " 'They': 47,\n",
       " 'beat': 48,\n",
       " 'ripped': 49,\n",
       " 'my': 50,\n",
       " 'clothes': 51,\n",
       " 'Yes': 52,\n",
       " 'we': 53,\n",
       " 'also': 54,\n",
       " 'say': 55,\n",
       " 'the': 56,\n",
       " 'European': 57,\n",
       " 'budget': 58,\n",
       " 'is': 59,\n",
       " 'not': 60,\n",
       " 'about': 61,\n",
       " 'duplication': 62,\n",
       " 'national': 63,\n",
       " 'budgets': 64,\n",
       " 'but': 65,\n",
       " 'delivering': 66,\n",
       " 'common': 67,\n",
       " 'goals': 68,\n",
       " 'beyond': 69,\n",
       " 'capacity': 70,\n",
       " 'nation': 71,\n",
       " 'states': 72,\n",
       " 'where': 73,\n",
       " 'funds': 74,\n",
       " 'can': 75,\n",
       " 'realise': 76,\n",
       " 'economies': 77,\n",
       " 'scale': 78,\n",
       " 'or': 79,\n",
       " 'create': 80,\n",
       " 'synergies': 81,\n",
       " 'The': 82,\n",
       " 'name': 83,\n",
       " 'this': 84,\n",
       " 'site': 85,\n",
       " 'program': 86,\n",
       " 'Title': 87,\n",
       " 'purchased': 88,\n",
       " 'be': 89,\n",
       " 'displayed': 90,\n",
       " 'would': 91,\n",
       " 'abiding': 92,\n",
       " 'by': 93,\n",
       " 'principle': 94,\n",
       " 'UN': 95,\n",
       " 'which': 96,\n",
       " 'precludes': 97,\n",
       " 'military': 98,\n",
       " 'action': 99,\n",
       " 'except': 100,\n",
       " 'in': 101,\n",
       " 'self-defence': 102,\n",
       " 'does': 103,\n",
       " 'apply': 104,\n",
       " 'here': 105,\n",
       " 'rapporteur': 106,\n",
       " '-': 107,\n",
       " '(FR)': 108,\n",
       " 'Mr': 109,\n",
       " 'President': 110,\n",
       " 'representatives': 111,\n",
       " 'Council': 112,\n",
       " 'Commission': 113,\n",
       " 'ladies': 114,\n",
       " 'gentlemen': 115,\n",
       " 'should': 116,\n",
       " 'like': 117,\n",
       " 'begin': 118,\n",
       " 'thanking': 119,\n",
       " 'colleagues': 120,\n",
       " 'who': 121,\n",
       " 'entrusted': 122,\n",
       " 'report': 123,\n",
       " 'shadow': 124,\n",
       " 'for': 125,\n",
       " 'their': 126,\n",
       " 'respective': 127,\n",
       " 'contributions': 128,\n",
       " 'Shortly': 129,\n",
       " 'thereafter': 130,\n",
       " 'Mårthen': 131,\n",
       " 'Cedergran': 132,\n",
       " 'had': 133,\n",
       " 'responsible': 134,\n",
       " 'vocals': 135,\n",
       " 'left': 136,\n",
       " 'Bombshell': 137,\n",
       " 'Rocks': 138,\n",
       " 'establish': 139,\n",
       " 'himself': 140,\n",
       " 'a': 141,\n",
       " 'tattoo': 142,\n",
       " 'artist': 143,\n",
       " 'next': 144,\n",
       " 'item': 145,\n",
       " 'presentation': 146,\n",
       " 'preliminary': 147,\n",
       " 'draft': 148,\n",
       " '2001': 149,\n",
       " 'This': 150,\n",
       " 'much': 151,\n",
       " 'hope': 152,\n",
       " 'within': 153,\n",
       " 'confines': 154,\n",
       " 'fisheries': 155,\n",
       " 'policy': 156,\n",
       " 'Description:': 157,\n",
       " 'ATTENTION': 158,\n",
       " 'PLEASE:': 159,\n",
       " 'WE': 160,\n",
       " 'CHARGE': 161,\n",
       " 'THE': 162,\n",
       " 'COST': 163,\n",
       " 'OF': 164,\n",
       " 'FIRST': 165,\n",
       " 'NIGHT': 166,\n",
       " 'AS': 167,\n",
       " 'A': 168,\n",
       " 'DEPOSIT': 169,\n",
       " 'AFTER': 170,\n",
       " 'ONLINE': 171,\n",
       " 'CONFIRMATION': 172,\n",
       " '!': 173,\n",
       " 'itself': 174,\n",
       " 'given': 175,\n",
       " 'assurances': 176,\n",
       " 'it': 177,\n",
       " 'see': 178,\n",
       " 'seafarers': 179,\n",
       " 'are': 180,\n",
       " 'put': 181,\n",
       " 'any': 182,\n",
       " 'disadvantage': 183,\n",
       " 'agreement': 184,\n",
       " 'In': 185,\n",
       " 'two': 186,\n",
       " 'days': 187,\n",
       " 'try': 188,\n",
       " 'Committee': 189,\n",
       " 'on': 190,\n",
       " 'Foreign': 191,\n",
       " 'Affairs': 192,\n",
       " 'Security': 193,\n",
       " 'Defence': 194,\n",
       " 'Policy': 195,\n",
       " 'our': 196,\n",
       " 'distinguished': 197,\n",
       " 'guests': 198,\n",
       " 'work': 199,\n",
       " 'out': 200,\n",
       " 'what': 201,\n",
       " 'framework': 202,\n",
       " 'Euro-Mediterranean': 203,\n",
       " 'Parliamentary': 204,\n",
       " 'Forum': 205,\n",
       " 'look': 206,\n",
       " 'Discover': 207,\n",
       " 'most': 208,\n",
       " 'intuitive': 209,\n",
       " 'design': 210,\n",
       " 'best': 211,\n",
       " 'image': 212,\n",
       " 'processing': 213,\n",
       " 'tools': 214,\n",
       " 'problem': 215,\n",
       " 'despite': 216,\n",
       " 'beautiful': 217,\n",
       " 'coast': 218,\n",
       " 'mountains': 219,\n",
       " 'good': 220,\n",
       " 'transparent': 221,\n",
       " 'progress': 222,\n",
       " 'made': 223,\n",
       " 'having': 224,\n",
       " 'environmental': 225,\n",
       " 'assessments': 226,\n",
       " 'then': 227,\n",
       " 'countryside': 228,\n",
       " 'danger': 229,\n",
       " 'being': 230,\n",
       " 'destroyed': 231,\n",
       " 'Ventura': 232,\n",
       " 'MC': 233,\n",
       " 'processes': 234,\n",
       " 'even': 235,\n",
       " 'demanding': 236,\n",
       " 'product': 237,\n",
       " 'at': 238,\n",
       " 'speed': 239,\n",
       " '200': 240,\n",
       " 'cycles': 241,\n",
       " 'per': 242,\n",
       " 'minute': 243,\n",
       " 'you': 244,\n",
       " 'call': 245,\n",
       " 'efficient': 246,\n",
       " 'book': 247,\n",
       " 'sewing': 248,\n",
       " 'everyday': 249,\n",
       " 'life': 250,\n",
       " 'works': 251,\n",
       " 'process': 252,\n",
       " 'improver': 253,\n",
       " 'Eiffel': 254,\n",
       " 'If': 255,\n",
       " 'conceded': 256,\n",
       " 'step': 257,\n",
       " 'forwards;': 258,\n",
       " 'none': 259,\n",
       " 'rest': 260,\n",
       " 'more': 261,\n",
       " 'effectiveness': 262,\n",
       " 'better': 263,\n",
       " 'organisation': 264,\n",
       " 'make': 265,\n",
       " 'difference': 266,\n",
       " 'Union': 267,\n",
       " '&apos;s': 268,\n",
       " 'decision-making': 269,\n",
       " 'He': 270,\n",
       " 'claimed': 271,\n",
       " 'his': 272,\n",
       " 'actions': 273,\n",
       " 'were': 274,\n",
       " 'justified': 275,\n",
       " 'because': 276,\n",
       " 'Cardassians': 277,\n",
       " 'preparing': 278,\n",
       " 'renew': 279,\n",
       " 'hostilities': 280,\n",
       " 'only': 281,\n",
       " 'year': 282,\n",
       " 'after': 283,\n",
       " 'peace': 284,\n",
       " 'settled': 285,\n",
       " 'United': 286,\n",
       " 'Planets': 287,\n",
       " 'page': 288,\n",
       " 'was': 289,\n",
       " 'last': 290,\n",
       " 'edited': 291,\n",
       " '10:': 292,\n",
       " '59': 293,\n",
       " '19': 294,\n",
       " 'June': 295,\n",
       " '2009': 296,\n",
       " 'Anonymous': 297,\n",
       " 'user': 298,\n",
       " '(s)': 299,\n",
       " 'Wikitravel': 300,\n",
       " 'Some': 301,\n",
       " 'wine-growers': 302,\n",
       " 'therefore': 303,\n",
       " 'tempted': 304,\n",
       " 'replant': 305,\n",
       " 'vineyards': 306,\n",
       " 'before': 307,\n",
       " 'date': 308,\n",
       " 'right': 309,\n",
       " 'do': 310,\n",
       " 'so': 311,\n",
       " 'expires': 312,\n",
       " 'To': 313,\n",
       " 'frank': 314,\n",
       " 'during': 315,\n",
       " 'negotiations': 316,\n",
       " 'support': 317,\n",
       " 'moreover': 318,\n",
       " 'became': 319,\n",
       " 'very': 320,\n",
       " 'clear': 321,\n",
       " 'us': 322,\n",
       " 'troublemakers': 323,\n",
       " 'honest': 324,\n",
       " 'access': 325,\n",
       " 'part': 326,\n",
       " 'organisations': 327,\n",
       " 'an': 328,\n",
       " 'arbitrary': 329,\n",
       " 'basis': 330,\n",
       " 'On': 331,\n",
       " 'find': 332,\n",
       " 'information': 333,\n",
       " 'trade': 334,\n",
       " 'fairs': 335,\n",
       " 'exhibiting': 336,\n",
       " 'many': 337,\n",
       " 'ways': 338,\n",
       " 'enlargement': 339,\n",
       " 'comparable': 340,\n",
       " 'tour': 341,\n",
       " 'de': 342,\n",
       " 'France': 343,\n",
       " 'theorists': 344,\n",
       " 'include': 345,\n",
       " 'formulation': 346,\n",
       " 'perceived': 347,\n",
       " 'distance': 348,\n",
       " 'moon': 349,\n",
       " '(how': 350,\n",
       " 'far': 351,\n",
       " 'appears': 352,\n",
       " 'observer).': 353,\n",
       " 'quality': 354,\n",
       " 'all': 355,\n",
       " 'furniture': 356,\n",
       " 'fixtures': 357,\n",
       " 'fittings': 358,\n",
       " 'excellent': 359,\n",
       " 'Bone': 360,\n",
       " 'Bruise': 361,\n",
       " 'Technique': 362,\n",
       " 'developed': 363,\n",
       " '1995': 364,\n",
       " 'used': 365,\n",
       " 'anywhere': 366,\n",
       " 'body': 367,\n",
       " 'throughout': 368,\n",
       " 'skeletal': 369,\n",
       " 'system': 370,\n",
       " 'correction': 371,\n",
       " 'acute': 372,\n",
       " 'chronic': 373,\n",
       " 'bone': 374,\n",
       " 'joint': 375,\n",
       " 'pain': 376,\n",
       " 'may': 377,\n",
       " 'related': 378,\n",
       " 'bruises': 379,\n",
       " 'Yet': 380,\n",
       " 'really': 381,\n",
       " 'want': 382,\n",
       " 'growth': 383,\n",
       " 'Europe': 384,\n",
       " 'genuinely': 385,\n",
       " 'need': 386,\n",
       " 'have': 387,\n",
       " 'single': 388,\n",
       " 'market': 389,\n",
       " 'compromise': 390,\n",
       " 'finally': 391,\n",
       " 'reached': 392,\n",
       " 'broadly': 393,\n",
       " 'speaking': 394,\n",
       " 'line': 395,\n",
       " 'Parliament': 396,\n",
       " '&apos;': 397,\n",
       " 's': 398,\n",
       " 'stance': 399,\n",
       " 'modified': 400,\n",
       " '15:': 401,\n",
       " '34': 402,\n",
       " '11': 403,\n",
       " 'September': 404,\n",
       " '2007': 405,\n",
       " 'Red': 406,\n",
       " '_': 407,\n",
       " 'marquis': 408,\n",
       " 'All': 409,\n",
       " 'detectives': 410,\n",
       " 'former': 411,\n",
       " 'workers': 412,\n",
       " 'special': 413,\n",
       " 'regiment': 414,\n",
       " 'Czech': 415,\n",
       " 'Police': 416,\n",
       " 'participated': 417,\n",
       " 'solution': 418,\n",
       " 'serious': 419,\n",
       " 'criminal': 420,\n",
       " 'offences': 421,\n",
       " 'Equally': 422,\n",
       " 'provisions': 423,\n",
       " 'readmission': 424,\n",
       " 'illegal': 425,\n",
       " 'immigrants': 426,\n",
       " 'while': 427,\n",
       " 'developing': 428,\n",
       " 'cooperation': 429,\n",
       " 'Iranian': 430,\n",
       " 'authorities': 431,\n",
       " 'broader': 432,\n",
       " 'issues': 433,\n",
       " 'migration': 434,\n",
       " 'Hallmarking': 435,\n",
       " '\"': 436,\n",
       " 'earliest': 437,\n",
       " 'form': 438,\n",
       " 'consumer': 439,\n",
       " 'protection': 440,\n",
       " 'probably': 441,\n",
       " 'started': 442,\n",
       " 'standard': 443,\n",
       " 'silver': 444,\n",
       " 'established': 445,\n",
       " '1260': 446,\n",
       " 'Apparently': 447,\n",
       " 'discussing': 448,\n",
       " 'paper': 449,\n",
       " 'Member': 450,\n",
       " 'States': 451,\n",
       " 'without': 452,\n",
       " 'informing': 453,\n",
       " 'doing': 454,\n",
       " 'Commissioner': 455,\n",
       " 'Fischler': 456,\n",
       " 'rural': 457,\n",
       " 'development': 458,\n",
       " 'must': 459,\n",
       " 'tackle': 460,\n",
       " 'three': 461,\n",
       " 'different': 462,\n",
       " 'angles:': 463,\n",
       " 'preservation': 464,\n",
       " 'jobs': 465,\n",
       " 'stewardship': 466,\n",
       " 'least': 467,\n",
       " 'local': 468,\n",
       " 'culture': 469,\n",
       " 'utmost': 470,\n",
       " 'importance': 471,\n",
       " 'Who': 472,\n",
       " 'going': 473,\n",
       " 'define': 474,\n",
       " 'how': 475,\n",
       " 'something': 476,\n",
       " 'intervention': 477,\n",
       " 'necessary': 478,\n",
       " '?': 479,\n",
       " 'think': 480,\n",
       " 'EU': 481,\n",
       " 'prepared': 482,\n",
       " 'face': 483,\n",
       " 'attack': 484,\n",
       " 'accident': 485,\n",
       " 'jeopardises': 486,\n",
       " 'Europeans': 487,\n",
       " 'security': 488,\n",
       " 'health': 489,\n",
       " 'We': 490,\n",
       " 'know': 491,\n",
       " 'customers': 492,\n",
       " 'often': 493,\n",
       " 'chosen': 494,\n",
       " 'Embedded': 495,\n",
       " 'Linux': 496,\n",
       " 'escape': 497,\n",
       " 'vendor': 498,\n",
       " 'lock-in': 499,\n",
       " 'However': 500,\n",
       " 'things': 501,\n",
       " 'quite': 502,\n",
       " 'opinion': 503,\n",
       " 'definitely': 504,\n",
       " 'viewed': 505,\n",
       " 'critical': 506,\n",
       " 'comments': 507,\n",
       " 'Australian': 508,\n",
       " 'Premier': 509,\n",
       " 'John': 510,\n",
       " 'Howard': 511,\n",
       " 'likewise': 512,\n",
       " 'openly': 513,\n",
       " 'mulled': 514,\n",
       " 'pre-emptive': 515,\n",
       " 'strikes': 516,\n",
       " 'terrorist': 517,\n",
       " 'sanctuaries': 518,\n",
       " 'region': 519,\n",
       " 'Later': 520,\n",
       " 'inhabited': 521,\n",
       " 'Kares': 522,\n",
       " 'Finikes': 523,\n",
       " 'Cretans': 524,\n",
       " 'Ionians': 525,\n",
       " 'Secondly': 526,\n",
       " 'financial': 527,\n",
       " 'assistance': 528,\n",
       " 'main': 529,\n",
       " 'ACP': 530,\n",
       " 'banana-producing': 531,\n",
       " 'countries': 532,\n",
       " 'Further': 533,\n",
       " 'inventions': 534,\n",
       " 'two-phase': 535,\n",
       " 'induction': 536,\n",
       " 'motor': 537,\n",
       " 'three-phase': 538,\n",
       " 'current': 539,\n",
       " 'multi-phase': 540,\n",
       " 'engines': 541,\n",
       " 'based': 542,\n",
       " 'areas': 543,\n",
       " 'energy': 544,\n",
       " 'magnetism': 545,\n",
       " 'weak': 546,\n",
       " 'power': 547,\n",
       " 'production': 548,\n",
       " 'transfer': 549,\n",
       " 'Going': 550,\n",
       " 'north': 551,\n",
       " 'Sun': 552,\n",
       " 'Yat-sen': 553,\n",
       " 'Freeway': 554,\n",
       " 'exit': 555,\n",
       " 'BinJiang': 556,\n",
       " 'Street': 557,\n",
       " 'Taipei': 558,\n",
       " 'turn': 559,\n",
       " 'end': 560,\n",
       " 'ramp': 561,\n",
       " 'few': 562,\n",
       " 'meters': 563,\n",
       " 'cross': 564,\n",
       " 'DaZhih': 565,\n",
       " 'Bridge': 566,\n",
       " 'writing': 567,\n",
       " '(IT)': 568,\n",
       " 'Madam': 569,\n",
       " 'voted': 570,\n",
       " 'favour': 571,\n",
       " 'Cottigny': 572,\n",
       " 'consulting': 573,\n",
       " 'employees': 574,\n",
       " 'Community': 575,\n",
       " 'That': 576,\n",
       " 'means': 577,\n",
       " '12': 578,\n",
       " '18': 579,\n",
       " 'months': 580,\n",
       " 'payment': 581,\n",
       " 'minimum': 582,\n",
       " 'subscription': 583,\n",
       " 'fine': 584,\n",
       " 'salt': 585,\n",
       " 'trust': 586,\n",
       " 'am': 587,\n",
       " 'astounded': 588,\n",
       " 'able': 589,\n",
       " 'accept': 590,\n",
       " '&apos;m': 591,\n",
       " 'interested': 592,\n",
       " 'girls': 593,\n",
       " 'four': 594,\n",
       " 'layers': 595,\n",
       " 'although': 596,\n",
       " 'pretty': 597,\n",
       " 'adorable': 598,\n",
       " 'bloom': 599,\n",
       " 'period': 600,\n",
       " 'nang': 601,\n",
       " 'sexy': 602,\n",
       " 'possible': 603,\n",
       " 'accommodate': 604,\n",
       " 'grandparents': 605,\n",
       " 'grown-up': 606,\n",
       " 'children': 607,\n",
       " 'separate': 608,\n",
       " 'apartments': 609,\n",
       " 'why': 610,\n",
       " 'actually': 611,\n",
       " 'kind': 612,\n",
       " 'virtual': 613,\n",
       " 'debate': 614,\n",
       " 'silly': 615,\n",
       " 'boot': 616,\n",
       " 'document': 617,\n",
       " 'CAP': 618,\n",
       " 'reform': 619,\n",
       " 'adequate': 620,\n",
       " 'reliable': 621,\n",
       " 'new': 622,\n",
       " 'measures': 623,\n",
       " 'instruments': 624,\n",
       " 'farmers': 625,\n",
       " 'improving': 626,\n",
       " 'crop': 627,\n",
       " 'rotation': 628,\n",
       " 'systems': 629,\n",
       " 'substantially': 630,\n",
       " 'reduce': 631,\n",
       " 'protein': 632,\n",
       " 'deficit': 633,\n",
       " 'price': 634,\n",
       " 'volatility': 635,\n",
       " 'Originals': 636,\n",
       " 'seen': 637,\n",
       " 'wonderful': 638,\n",
       " 'rooms': 639,\n",
       " 'Austrian': 640,\n",
       " 'Gallery': 641,\n",
       " 'Belvedere': 642,\n",
       " 'Vienna': 643,\n",
       " 'indicators': 644,\n",
       " 'digital': 645,\n",
       " 'marine': 646,\n",
       " 'charts': 647,\n",
       " 'drawn': 648,\n",
       " 'cultural': 649,\n",
       " 'purposes': 650,\n",
       " 'tricky': 651,\n",
       " 'situation': 652,\n",
       " 'fundamentally': 653,\n",
       " 'important': 654,\n",
       " 'give': 655,\n",
       " 'signal': 656,\n",
       " 'unity': 657,\n",
       " 'responsibility': 658,\n",
       " 'way': 659,\n",
       " 'get': 660,\n",
       " 'set': 661,\n",
       " 'emission': 662,\n",
       " 'reduction': 663,\n",
       " 'targets': 664,\n",
       " 'post-Kyoto': 665,\n",
       " '(2012)': 666,\n",
       " 'off': 667,\n",
       " 'start': 668,\n",
       " 'Finally': 669,\n",
       " 'third': 670,\n",
       " 'lesson': 671,\n",
       " 'relevant': 672,\n",
       " 'fight': 673,\n",
       " 'against': 674,\n",
       " 'death': 675,\n",
       " 'penalty': 676,\n",
       " 'indeed': 677,\n",
       " 'winning': 678,\n",
       " 'political': 679,\n",
       " 'battle': 680,\n",
       " 'perseverance': 681,\n",
       " 'consider': 682,\n",
       " 'mix': 683,\n",
       " 'pig-headedness': 684,\n",
       " 'ability': 685,\n",
       " 'maintain': 686,\n",
       " 'one': 687,\n",
       " 'position': 688,\n",
       " 'And': 689,\n",
       " 'case': 690,\n",
       " 'appeal': 691,\n",
       " 'decision': 692,\n",
       " 'imprison': 693,\n",
       " 'Spanish': 694,\n",
       " 'businessman': 695,\n",
       " 'Helms-Burton': 696,\n",
       " 'Act': 697,\n",
       " 'With': 698,\n",
       " 'recent': 699,\n",
       " 'plan': 700,\n",
       " 'ECU': 701,\n",
       " '10m': 702,\n",
       " 'slightly': 703,\n",
       " 'enlarged': 704,\n",
       " 'list': 705,\n",
       " 'NGOs': 706,\n",
       " 'Look': 707,\n",
       " 'happens': 708,\n",
       " 'Switzerland': 709,\n",
       " 'taxes': 710,\n",
       " 'same': 711,\n",
       " 'Smiths': 712,\n",
       " 'Jervis': 713,\n",
       " 'St': 714,\n",
       " 'Dublin': 715,\n",
       " '1.': 716,\n",
       " 'pub': 717,\n",
       " 'food': 718,\n",
       " 'handy': 719,\n",
       " 'stop': 720,\n",
       " '&apos;re': 721,\n",
       " 'shopping': 722,\n",
       " 'around': 723,\n",
       " 'Henry': 724,\n",
       " 'area': 725,\n",
       " 'Among': 726,\n",
       " 'winter': 727,\n",
       " 'sports': 728,\n",
       " 'formula': 729,\n",
       " 'Nature': 730,\n",
       " 'those': 731,\n",
       " 'love': 732,\n",
       " 'cross-country': 733,\n",
       " 'skiing': 734,\n",
       " 'EMULE': 735,\n",
       " 'electronic': 736,\n",
       " 'mule': 737,\n",
       " 'appropriately': 738,\n",
       " 'named': 739,\n",
       " 'powerful': 740,\n",
       " 'file': 741,\n",
       " 'sharing': 742,\n",
       " 'over': 743,\n",
       " 'couple': 744,\n",
       " 'years': 745,\n",
       " 'opportunity': 746,\n",
       " 'perhaps': 747,\n",
       " 'alas': 748,\n",
       " 'making': 749,\n",
       " 'too': 750,\n",
       " 'statements': 751,\n",
       " 'Middle': 752,\n",
       " 'East': 753,\n",
       " 'Known': 754,\n",
       " 'Typhoon': 755,\n",
       " 'export': 756,\n",
       " 'high-performance': 757,\n",
       " 'multi-role': 758,\n",
       " 'combat': 759,\n",
       " 'aircraft': 760,\n",
       " 'designed': 761,\n",
       " 'primarily': 762,\n",
       " 'air': 763,\n",
       " 'superiority': 764,\n",
       " 'missions': 765,\n",
       " 'extended': 766,\n",
       " 'air-to-ground': 767,\n",
       " 'mission': 768,\n",
       " 'capability': 769,\n",
       " 'web': 770,\n",
       " 'born': 771,\n",
       " 'curiosity': 772,\n",
       " 'its': 773,\n",
       " 'goal': 774,\n",
       " 'gather': 775,\n",
       " 'informative': 776,\n",
       " 'statistical': 777,\n",
       " 'data': 778,\n",
       " 'regarding': 779,\n",
       " 'delays': 780,\n",
       " 'Cisalpino': 781,\n",
       " 'Pendolino': 782,\n",
       " 'Why': 783,\n",
       " 'internet': 784,\n",
       " 'connection': 785,\n",
       " 'slowly': 786,\n",
       " 'As': 787,\n",
       " 'speak': 788,\n",
       " 'country': 789,\n",
       " 'Italy': 790,\n",
       " 'increased': 791,\n",
       " 'arms': 792,\n",
       " 'exports': 793,\n",
       " '25%': 794,\n",
       " 'forecasting': 795,\n",
       " '40%': 796,\n",
       " 'increase': 797,\n",
       " 'future': 798,\n",
       " 'Born': 799,\n",
       " '1958': 800,\n",
       " 'Obernberg': 801,\n",
       " 'Inn': 802,\n",
       " '/': 803,\n",
       " 'Austria': 804,\n",
       " 'It': 805,\n",
       " 'warmly': 806,\n",
       " 'expressive': 807,\n",
       " 'intensely': 808,\n",
       " 'phrased': 809,\n",
       " 'recorded': 810,\n",
       " 'sound': 811,\n",
       " 'full': 812,\n",
       " 'rich': 813,\n",
       " '(If': 814,\n",
       " 'abilities': 815,\n",
       " 'use': 816,\n",
       " 'creature': 817,\n",
       " 'instead': 818,\n",
       " 'absolutely': 819,\n",
       " 'crucial': 820,\n",
       " 'entirely': 821,\n",
       " 'agree': 822,\n",
       " 'Mrs': 823,\n",
       " 'Mann': 824,\n",
       " 'concerned': 825,\n",
       " 'From': 826,\n",
       " 'outset': 827,\n",
       " 'leader': 828,\n",
       " 'field': 829,\n",
       " 'installation': 830,\n",
       " 'greenhousing': 831,\n",
       " 'products': 832,\n",
       " 'both': 833,\n",
       " 'international': 834,\n",
       " 'level': 835,\n",
       " 'At': 836,\n",
       " 'present': 837,\n",
       " 'run': 838,\n",
       " 'Spelgatti': 839,\n",
       " 'family': 840,\n",
       " 'For': 841,\n",
       " 'than': 842,\n",
       " '60': 843,\n",
       " 'garden': 844,\n",
       " 'care': 845,\n",
       " 'fact': 846,\n",
       " 'question': 847,\n",
       " 'others': 848,\n",
       " 'said': 849,\n",
       " 'stress': 850,\n",
       " 'point': 851,\n",
       " 'nothing': 852,\n",
       " 'happen': 853,\n",
       " 'Keeping': 854,\n",
       " 'mage': 855,\n",
       " 'warlock': 856,\n",
       " 'feeling': 857,\n",
       " 'distinct': 858,\n",
       " 'big': 859,\n",
       " 'challenge': 860,\n",
       " 'bar': 861,\n",
       " 'great': 862,\n",
       " 'place': 863,\n",
       " 'meet': 864,\n",
       " 'talk': 865,\n",
       " 'just': 866,\n",
       " 'enjoy': 867,\n",
       " 'surroundings': 868,\n",
       " 'mean': 869,\n",
       " 'definition': 870,\n",
       " 'constitutes': 871,\n",
       " 'crime': 872,\n",
       " 'completely': 873,\n",
       " 'terms': 874,\n",
       " 'type': 875,\n",
       " 'Conan': 876,\n",
       " 'Doyle': 877,\n",
       " 'spent': 878,\n",
       " 'holidays': 879,\n",
       " 'Meiringen': 880,\n",
       " 'there': 881,\n",
       " 'museum': 882,\n",
       " 'dedicated': 883,\n",
       " 'him': 884,\n",
       " 'famous': 885,\n",
       " 'detective': 886,\n",
       " 'Readers': 887,\n",
       " 'reacted': 888,\n",
       " 'news': 889,\n",
       " 'Holmes': 890,\n",
       " 'displeasure': 891,\n",
       " 'led': 892,\n",
       " 'novelist': 893,\n",
       " 'resurrect': 894,\n",
       " 'later': 895,\n",
       " 'population': 896,\n",
       " 'expects': 897,\n",
       " 'such': 898,\n",
       " 'obvious': 899,\n",
       " 'problems': 900,\n",
       " 'solved': 901,\n",
       " '*': 902,\n",
       " 'JBOD': 903,\n",
       " 'Just': 904,\n",
       " 'bunch': 905,\n",
       " 'discs': 906,\n",
       " '\":': 907,\n",
       " 'PC': 908,\n",
       " 'recognize': 909,\n",
       " 'several': 910,\n",
       " 'HDDs': 911,\n",
       " 'via': 912,\n",
       " 'USB': 913,\n",
       " 'cable': 914,\n",
       " 'But': 915,\n",
       " 'Zeroual': 916,\n",
       " 'government': 917,\n",
       " 'take': 918,\n",
       " 'Algeria': 919,\n",
       " 'rut': 920,\n",
       " 'fallen': 921,\n",
       " 'into': 922,\n",
       " ')': 923,\n",
       " 'first': 924,\n",
       " 'floor:': 925,\n",
       " 'living': 926,\n",
       " 'room': 927,\n",
       " 'fire': 928,\n",
       " 'brick': 929,\n",
       " 'cooking': 930,\n",
       " 'corner': 931,\n",
       " 'dischwascher': 932,\n",
       " 'double': 933,\n",
       " 'divan-bed': 934,\n",
       " '2': 935,\n",
       " 'bath': 936,\n",
       " 'shower': 937,\n",
       " 'satellite': 938,\n",
       " 'tv': 939,\n",
       " 'adventuresome': 940,\n",
       " 'vibrant': 941,\n",
       " 'metropolis': 942,\n",
       " 'perfect': 943,\n",
       " 'travel': 944,\n",
       " 'destination': 945,\n",
       " 'centre': 946,\n",
       " 'carnival': 947,\n",
       " 'always': 948,\n",
       " 'worth': 949,\n",
       " 'journey': 950,\n",
       " 'plugin': 951,\n",
       " 'scans': 952,\n",
       " 'pages': 953,\n",
       " 'PDF': 954,\n",
       " 'links': 955,\n",
       " 'open': 956,\n",
       " 'Google': 957,\n",
       " 'Docs': 958,\n",
       " 'Viewer': 959,\n",
       " 'So': 960,\n",
       " 'no': 961,\n",
       " 'Adobe': 962,\n",
       " 'reader': 963,\n",
       " 'outcome': 964,\n",
       " 'abovementioned': 965,\n",
       " 'dialogue': 966,\n",
       " 'result': 967,\n",
       " 'paving': 968,\n",
       " 'towards': 969,\n",
       " 'emphasise': 970,\n",
       " 'modernised': 971,\n",
       " 'providing': 972,\n",
       " 'equal': 973,\n",
       " 'conditions': 974,\n",
       " 'laser': 975,\n",
       " 'light': 976,\n",
       " 'develop': 977,\n",
       " 'high': 978,\n",
       " 'precision': 979,\n",
       " 'measurements': 980,\n",
       " 'factory': 981,\n",
       " 'on-line': 982,\n",
       " 'bench': 983,\n",
       " 'enables': 984,\n",
       " 'easier': 985,\n",
       " 'handling': 986,\n",
       " 'when': 987,\n",
       " 'bag': 988,\n",
       " 'ground': 989,\n",
       " 'lid': 990,\n",
       " 'your': 991,\n",
       " 'further': 992,\n",
       " 'results': 993,\n",
       " 'still': 994,\n",
       " 'fundamental': 995,\n",
       " 'hotels': 996,\n",
       " 'restaurant': 997,\n",
       " 'El': 998,\n",
       " 'Rincon': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dictionary.token_index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05c9ee",
   "metadata": {},
   "source": [
    "# 最后一块的，准备数据集的最后一块了，但完全没有看的干劲啊啊啊啊啊啊啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd2ed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTargetTranslationDataset:\n",
    "    #在IndexedInputTargetTranslationDataset的prepare中就用到了\n",
    "    def __init__(self, data_dir, phase, limit=None):\n",
    "        #phase值为'train' 或者 'val'\n",
    "        self.tokenized_dataset = TokenizedTranslationDataset(data_dir, phase, limit)\n",
    "        #对字符串进行切割的类，在获取元素的时候切割为list\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokenized_source, tokenized_target = self.tokenized_dataset[item]#获取切割后的字符list\n",
    "        #target是德语，source是英语\n",
    "        #source的词汇表映射的结果都是1，为什么呢。哦，是没有运行完23333，英语词汇表只有UNK啊，那不得都是1了\n",
    "        #运行完全就正常了2333\n",
    "        full_target = [START_TOKEN] + tokenized_target + [END_TOKEN]#加上开始和结束的字符\n",
    "        inputs = full_target[:-1]#输入去掉了最后一个停止符\n",
    "        targets = full_target[1:]#目标去掉了最前面的开始符号\n",
    "        return tokenized_source, inputs, targets \n",
    "        #返回的是英语的切割后的字符串列表，去掉了最后的停止符的德语，去掉了开始符号的德语。\n",
    "        #原因还没有理解\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1366e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_INDEX = 1#词汇表未知的词的编码\n",
    "\n",
    "class IndexedInputTargetTranslationDataset:\n",
    "    #首先调用的是这个类\n",
    "    def __init__(self, data_dir, phase, vocabulary_size=None, limit=None):\n",
    "        #prepare完全后，得到编码后的结果文件，后就是从文件中获取数据了\n",
    "        self.data = []#数据都在data里了，[(source,input,target)]\n",
    "\n",
    "        unknownify = lambda index: index if index < vocabulary_size else UNK_INDEX\n",
    "        #其输入参数为 index，表示一个单词在词典中的下标；返回值为 index，如果该下标小于词汇表大小，\n",
    "        #否则返回 UNK_INDEX，表示未出现在词典中的单词。\n",
    "        with open(join(data_dir, f'indexed-{phase}.txt'), encoding='utf-8') as file:#读取文件\n",
    "            for line in file:\n",
    "                sources, inputs, targets = line.strip().split('\\t')#英文，带开始符的德文，带结束符的德文\n",
    "                if vocabulary_size is not None:#正常就跑这段\n",
    "                    indexed_sources = [unknownify(int(index)) for index in sources.strip().split(' ')]\n",
    "                    #做词汇表的未知符号判断，返回都转为int的数字了\n",
    "                    indexed_inputs = [unknownify(int(index)) for index in inputs.strip().split(' ')]\n",
    "                    indexed_targets = [unknownify(int(index)) for index in targets.strip().split(' ')]\n",
    "                else:\n",
    "                    indexed_sources = [int(index) for index in sources.strip().split(' ')]\n",
    "                    indexed_inputs = [int(index) for index in inputs.strip().split(' ')]\n",
    "                    indexed_targets = [int(index) for index in targets.strip().split(' ')]\n",
    "                self.data.append((indexed_sources, indexed_inputs, indexed_targets))\n",
    "                if limit is not None and len(self.data) >= limit:\n",
    "                    break\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.limit = limit\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.limit is not None and item >= self.limit:\n",
    "            raise IndexError()\n",
    "\n",
    "        indexed_sources, indexed_inputs, indexed_targets = self.data[item]\n",
    "        return indexed_sources, indexed_inputs, indexed_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.limit is None:\n",
    "            return len(self.data)\n",
    "        else:\n",
    "            return self.limit\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(source_dictionary):\n",
    "\n",
    "        def preprocess_function(source):\n",
    "            source_tokens = source.strip().split()\n",
    "            indexed_source = source_dictionary.index_sentence(source_tokens)\n",
    "            return indexed_source\n",
    "\n",
    "        return preprocess_function\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare(data_dir, source_dictionary, target_dictionary):\n",
    "    # 最先看的是这里，最先运行的是这个\n",
    "        join_indexes = lambda indexes: ' '.join(str(index) for index in indexes)\n",
    "        #这个函数接受一个参数`indexes`，作用是将`indexes`中的元素转换为字符串并用空格拼接起来。\n",
    "        for phase in ('train', 'val'):\n",
    "            input_target_dataset = InputTargetTranslationDataset(data_dir, phase)#返回的是三个东西\n",
    "\n",
    "            with open(join(data_dir, f'indexed-{phase}.txt'), 'w', encoding='utf-8') as file:\n",
    "                #保存文件到“indexed-train.txt”类似这样的文件名里面\n",
    "                for sources, inputs, targets in input_target_dataset:\n",
    "                    #英语的切割后的字符串列表，去掉了最后的停止符的德语，去掉了开始符号的德语。\n",
    "                    indexed_sources = join_indexes(source_dictionary.index_sentence(sources))#为什么source都是1\n",
    "                    #.index_sentence(sources)这步字符串映射为数字，差不多one-hot编码\n",
    "                    indexed_inputs = join_indexes(target_dictionary.index_sentence(inputs))\n",
    "                    indexed_targets = join_indexes(target_dictionary.index_sentence(targets))\n",
    "                    file.write(f'{indexed_sources}\\t{indexed_inputs}\\t{indexed_targets}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a3f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedInputTargetTranslationDatasetOnTheFly:\n",
    "\n",
    "    def __init__(self, phase, source_dictionary, target_dictionary, limit=None):\n",
    "\n",
    "        self.input_target_dataset = InputTargetTranslationDatasetOnTheFly(phase, limit)\n",
    "        self.source_dictionary = source_dictionary\n",
    "        self.target_dictionary = target_dictionary\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        source, inputs, targets = self.input_target_dataset[item]\n",
    "        indexed_source = self.source_dictionary.index_sentence(source)\n",
    "        indexed_inputs = self.target_dictionary.index_sentence(inputs)\n",
    "        indexed_targets = self.target_dictionary.index_sentence(targets)\n",
    "\n",
    "        return indexed_source, indexed_inputs, indexed_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_target_dataset)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(source_dictionary):\n",
    "\n",
    "        def preprocess_function(source):\n",
    "            source_tokens = source.strip().split()\n",
    "            indexed_source = source_dictionary.index_sentence(source_tokens)\n",
    "            return indexed_source\n",
    "\n",
    "        return preprocess_function\n",
    "\n",
    "class InputTargetTranslationDatasetOnTheFly:\n",
    "\n",
    "    def __init__(self, phase, limit=None):\n",
    "        self.tokenized_dataset = TokenizedTranslationDatasetOnTheFly(phase, limit)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokenized_source, tokenized_target = self.tokenized_dataset[item]\n",
    "        full_target = [START_TOKEN] + tokenized_target + [END_TOKEN]\n",
    "        inputs = full_target[:-1]\n",
    "        targets = full_target[1:]\n",
    "        return tokenized_source, inputs, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "    \n",
    "class TokenizedTranslationDatasetOnTheFly:\n",
    "\n",
    "    def __init__(self, phase, limit=None):\n",
    "\n",
    "        self.raw_dataset = TranslationDatasetOnTheFly(phase, limit)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        raw_source, raw_target = self.raw_dataset[item]\n",
    "        tokenized_source = raw_source.split()\n",
    "        tokenized_target = raw_target.split()\n",
    "        return tokenized_source, tokenized_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6816443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexedInputTargetTranslationDataset.prepare(save_data_dir, source_dictionary, target_dictionary)\n",
    "indexed_translation_dataset = IndexedInputTargetTranslationDataset(save_data_dir, 'train')\n",
    "#就是获取到int数据类型的编码后的数据\n",
    "\n",
    "indexed_translation_dataset_on_the_fly = IndexedInputTargetTranslationDatasetOnTheFly('train', source_dictionary, target_dictionary)\n",
    "#这个做一样的东西，就是没有数据预处理的，动态得从源数据文件中取读取数据去处理\n",
    "assert indexed_translation_dataset[0] == indexed_translation_dataset_on_the_fly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296b6fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([805,\n",
       "  59,\n",
       "  60,\n",
       "  7624,\n",
       "  38,\n",
       "  8,\n",
       "  21,\n",
       "  56,\n",
       "  1814,\n",
       "  5,\n",
       "  56,\n",
       "  63,\n",
       "  1,\n",
       "  8,\n",
       "  396,\n",
       "  268,\n",
       "  5497,\n",
       "  1,\n",
       "  116,\n",
       "  89,\n",
       "  223,\n",
       "  9547,\n",
       "  13,\n",
       "  1,\n",
       "  93,\n",
       "  577,\n",
       "  5,\n",
       "  2978,\n",
       "  423,\n",
       "  1617,\n",
       "  1920,\n",
       "  8,\n",
       "  6173,\n",
       "  13,\n",
       "  2409,\n",
       "  180,\n",
       "  60,\n",
       "  4686,\n",
       "  1377,\n",
       "  101,\n",
       "  4064,\n",
       "  29],\n",
       " [2,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  8,\n",
       "  12,\n",
       "  13,\n",
       "  8,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  6,\n",
       "  18,\n",
       "  19,\n",
       "  8,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32],\n",
       " [4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  8,\n",
       "  12,\n",
       "  13,\n",
       "  8,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  6,\n",
       "  18,\n",
       "  19,\n",
       "  8,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_translation_dataset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6eaeb",
   "metadata": {},
   "source": [
    "# 呼，结束，到下一步的文件去学习\n",
    "啦啦啦啦啦，四舍五入，结束1/3了，开森"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9385c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
